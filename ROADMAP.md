- [ ] Cover core with tests
- [x] Move main.go from root to other destination
- [ ] New layers and learning optimization
    - [ ] Softmax layer;
    - [ ] Maxout layer;
    - [ ] Dropout layer;
    - [ ] Optimization for learning;
    - [ ] Bias;
- [ ] Add new operations
    - [x] Convolve2D
    - [ ] Determinant
- [ ] Refactor code and rethink structure of library
- [ ] Remove some legacy and experemintal code to other branches **WIP**
- [ ] Error types and wrapping them **WIP**
- [ ] Test cases for most of math functions
    - [x] ActivationTanh
    - [x] ActivationTanhDerivative
    - [x] ActivationSygmoid
    - [x] ActivationSygmoidDerivative
    - [ ] Rot2D90
    - [ ] Rot2D180
    - [ ] Rot2D270
    - [x] ActivationArcTan
    - [x] ActivationArcTanDerivative
    - [x] ActivationSoftPlus
    - [x] ActivationSoftPlusDerivative
    - [x] ActivationGaussian
    - [x] ActivationGaussianDerivative
    - [x] Add (element-wise)
    - [x] Sub (element-wise)
    - [x] Transpose
    - [x] Multiply
    - [x] HadamardProduct
    - [ ] MSE
    - [x] Convolve2D
- [ ] Test cases for layers and its methods
    - [ ] Convolutional **WIP**
    - [ ] Fully connected **WIP**
    - [ ] ReLU
    - [ ] Leaky ReLU
    - [ ] Pooling  
- [ ] Benchmarks. Do we really need it since this is just library for studying purposes?
- [ ] Padding for convolutional layer
- [ ] Write theoretical documents on most of functions (on every would be even better)
- [x] New struct of examples folder (split it on different types of tasks for neural networks)
- [ ] Consider float32 as extension
- [ ] Improve README's **WIP**
- [ ] Graphviz pretty print. **WIP**
- [x] Add CI on https://travis-ci.com

Updated at: 2020-09-08